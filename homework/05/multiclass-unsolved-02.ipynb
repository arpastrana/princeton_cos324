{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 - Multiclass Learning\n",
    "\n",
    "In this exercise you are required to design, implement, train, and test multiclass learning algorithms for the [MNIST](http://yann.lecun.com/exdb/mnist) dataset of images and the [20 Newsgroups](http://qwone.com/~jason/20Newsgroups) dataset of text news data. You will explore two different approaches to multiclass learning: *Cross Entropy*, a general loss appropriate for multiple classes, and *One-vs-All*, a reduction scheme from binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have the packages numpy, scikit-learn, and Tensorflow\n",
    "# installed before starting the assignment\n",
    "\n",
    "import time\n",
    "# import numpy first\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Losses\n",
    "\n",
    "In the case of Cross Entropy, denoted in short as **CE**, the supervision for an example $\\mathbf{x}\\in\\mathbb{R}^d$  is, instead of a label\n",
    "$y\\in[k]\\stackrel{\\tiny \\mathsf{def}}{=}\\{1,\\ldots,k\\}$, a vector $\\mathbf{y}\\in\\Delta^k$ where\n",
    "$$\\Delta^k = \\big\\{\\mathbf{p}=(p_1,\\ldots,p_k)\\, \\big| \\, \\sum_i p_i = 1\n",
    "   \\mbox{ and } \\forall i: p_i \\geq 0 \\big\\} ~.$$\n",
    "$\\Delta^k$ is called the k$^{th}$ dimensional simplex and, informally speaking, it consists of all probability vectors in $\\mathbb{R}^k$. The CE loss for a target probability vector $\\mathbf{y}$ and predicted probability vector $\\mathbf{\\hat{y}}$ is defined as follows,\n",
    "   $$\\ell_{CE}(\\mathbf{y, \\hat{y}}) \\stackrel{\\tiny \\mathsf{def}}{=}\n",
    "     \\sum_{i=1}^k y[i] \\log\\left(\\frac{y[i]}{\\hat{y}[i]}\\right) ~ .$$\n",
    "We use the convention that $0\\log(0)\\stackrel{\\tiny \\mathsf{def}}{=} 0$. To obtain the vector $\\mathbf{\\hat{y}}$ we exponentiate the vector of predictions $\\mathbf{z} = W\\mathbf{x} \\in \\mathbb{R}^k$ where $\\mathbf{\\hat{y}}[i] = \\frac{\\exp({\\mathbf{z}[i]})}{Z}$ for all $i \\in [k]$ and $Z$ ensures that $\\mathbf{\\hat{y}}\\in \\Delta^k $.\n",
    "\n",
    "The One-vs-All, or **OvA**, approach instead uses $k$ logistic binary classifiers: the idea is to pick the label that has been *most confidently* chosen for the given example versus the rest. Formally, the loss can be expressed as follows. Given $(\\mathbf{x},y)\\in\\mathbb{R}^d\\times[k]$ let us define $\\mathbf{\\bar{y}}\\in\\{-1,1\\}^k$ as the vector of labels resulting from the multiclass reduction. That is, $\\mathbf{\\bar{y}}[y]=1$ and for all $j\\neq y$ we set $\\mathbf{\\bar{y}}[j]=-1$. Then the loss for $(\\mathbf{x},y)$, with $\\mathbf{z}$ defined as before, is\n",
    "$$\n",
    "\\ell_{OvA}(\\mathbf{\\bar{y}}, \\mathbf{z}) = \\sum_{j=1}^k \\log\\big(1+ e^{-\\mathbf{\\bar{y}}[j] \\, \\mathbf{z}[j]}\\big) ~ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z∈R is a constant number, and it is there to make sure that after exponentiation of the vector z the vector y^ belongs to Δk. More specifically, the constant Z ensures that ∑kj=1y^j=1.\n",
    "ACT 1 is asking you to provide an explicit formula for this constant Z (it will be in terms of the vector z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACT 1\n",
    "Compute the value of $Z$ explicitly.\n",
    "$$\n",
    "Z\n",
    "=\n",
    "\\sum_{i=1}^k\n",
    "\\exp({\\mathbf{z}[i]})\n",
    "$$\n",
    "\n",
    "To propose a method of computing $\\mathbf{\\hat{y}}$ from $\\mathbf{z}$ that is numerically stable, we should consider the scenarios where $\\mathbf{z}[i]<<0$ and $\\mathbf{z}[i]>>0$, which would result in underflow and overflow when doing the exponentiation, respectively.\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = \\frac{\\mathbf{z}}{||\\mathbf{z}||}\n",
    "$$\n",
    "\n",
    "Then, I would proceed as stated above to calculate $\\mathbf{\\hat{y}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "z [[686.22694787 998.37832871 315.29394029 598.26219467 951.54663015]]\n",
      "\n",
      "c 998.3783287139088\n",
      "bound z [[-312.15138084    0.         -683.08438842 -400.11613404  -46.83169857]]\n",
      "exp(bound z) [[2.71880326e-136 1.00000000e+000 2.18886730e-297 1.70518249e-174\n",
      "  4.58407521e-021]]\n",
      "\n",
      "y_hat [[2.71880326e-136 1.00000000e+000 2.18886730e-297 1.70518249e-174\n",
      "  4.58407521e-021]]\n",
      "sum y_hat 1.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_Z(z):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return np.sum(np.exp(z))\n",
    "\n",
    "def calculate_y_hat(z):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # z = bound_z(z)\n",
    "    Z = calculate_Z(z)\n",
    "    y = np.exp(z)\n",
    "    return y / Z\n",
    "\n",
    "def bound_z(z, axis=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    c = np.amax(z, axis=axis)\n",
    "    print('c', c)\n",
    "    return z - np.amax(z, axis=axis)    \n",
    "\n",
    "\n",
    "# testing ACT 1\n",
    "z = np.random.rand(1, 5)\n",
    "z = z * 1000\n",
    "\n",
    "print(z.shape)\n",
    "print('z', z)\n",
    "\n",
    "print()\n",
    "z_bound = bound_z(z)\n",
    "print('bound z', z_bound)\n",
    "\n",
    "print('exp(bound z)', np.exp(z_bound))\n",
    "\n",
    "z = z_bound\n",
    "\n",
    "print()\n",
    "y_hat = calculate_y_hat(z)\n",
    "print('y_hat', y_hat)\n",
    "\n",
    "print('sum y_hat', np.sum(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\in \\mathbb{R}^{n \\times d}$ be the given dataset and $\\mathbf{y} \\in [k]^n$ the corresponding labels. Construct one-hot vectors for each label to have a probability vector from $\\Delta^k$: the one-hot vector for the class $c \\in [k]$ if defined as $\\mathbf{1}_c \\in \\Delta^k$ which is a vector of zeros except for coordinate $c$ where it is $1$. For example, suppose $k=4$ and $c=2$, then $\\mathbf{1}_c$ is $(0, 1, 0, 0)$. When needed we will\n",
    "be smoothing the one-hot vectors for numerical purposes. The $\\epsilon$-smoothed one-hot vector for class $c$ is given by $(1-\\epsilon) \\mathbf{1}_c + \\epsilon \\mathbf{\\frac1k}$ where $\\mathbf{\\frac1k} \\in \\Delta^k$ is the uniform probability vector, namely $\\mathbf{\\frac1k} = (\\frac1k, \\frac1k, \\ldots, \\frac1k)$.\n",
    "\n",
    "ACT 2: Implement the `one_hot` function that given $\\mathbf{y} \\in [k]^n$ data of labels returns $Y \\in \\mathbb{R}^{n \\times k}$ matrix of one-hot vectors. **In the code, the labels $\\mathbf{y}$ are $0$-indexed, so you need to adjust accordingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [1]\n",
      " [4]]\n",
      "(10, 1)\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "[[2.000000e-07 2.000000e-07 9.999992e-01 2.000000e-07 2.000000e-07]\n",
      " [2.000000e-07 9.999992e-01 2.000000e-07 2.000000e-07 2.000000e-07]\n",
      " [9.999992e-01 2.000000e-07 2.000000e-07 2.000000e-07 2.000000e-07]\n",
      " [2.000000e-07 2.000000e-07 9.999992e-01 2.000000e-07 2.000000e-07]\n",
      " [2.000000e-07 2.000000e-07 9.999992e-01 2.000000e-07 2.000000e-07]\n",
      " [2.000000e-07 2.000000e-07 2.000000e-07 9.999992e-01 2.000000e-07]\n",
      " [2.000000e-07 2.000000e-07 2.000000e-07 9.999992e-01 2.000000e-07]\n",
      " [2.000000e-07 2.000000e-07 2.000000e-07 9.999992e-01 2.000000e-07]\n",
      " [2.000000e-07 9.999992e-01 2.000000e-07 2.000000e-07 2.000000e-07]\n",
      " [2.000000e-07 2.000000e-07 2.000000e-07 2.000000e-07 9.999992e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Given y vector of labels, return matrix Y whose i-th row is the one-hot vector of label y_i \n",
    "# If eps is not 0, then need to compute the smoothed one-hot vectors instead\n",
    "# If k is 0, then need to infer its value instead\n",
    "def one_hot(y, k=0, eps=0):\n",
    "    \"\"\"\n",
    "    ACT 2\n",
    "    \"\"\"\n",
    "    n, _ = y.shape  # assuming y has shape (1, n)\n",
    "    \n",
    "    if not k:\n",
    "        k = np.amax(y)  + 1\n",
    "    \n",
    "    Y = np.zeros((n, k))\n",
    "    col_indices = y[:, 0]\n",
    "    Y[np.arange(n), col_indices] = 1.0\n",
    "    \n",
    "    if eps:\n",
    "        Y = (1 - eps) * Y + (eps * 1 / k)\n",
    "    return Y\n",
    "\n",
    "\n",
    "# testing y hot\n",
    "y_test = np.random.randint(0, 5, size=(10, 1))\n",
    "print(y_test)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_hot = one_hot(y_test)\n",
    "y_hot.shape\n",
    "\n",
    "print(y_hot)\n",
    "\n",
    "y_hot = one_hot(y_test, eps=1e-6)\n",
    "print(y_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first implement an abstract class for losses in general that has all the methods implemented except computing the loss value and the gradient. Then each of CE and OvA classes will implement their specific loss functions, and gradients separately. *Feel free to add your own methods to the class skeletons provided.*\n",
    "\n",
    "It is worth to note that label prediction in both cases is done by predicting the coordinate with the largest value in the vector $\\mathbf{z}$ as the label. In order to prevent the parameters from blowing up during training, we will implement methods `RowNorms` and `Project` to compute the norms of the rows of $W \\in \\mathbb{R}^{k \\times d}$ and their projections onto the sphere of radius $r$.\n",
    "\n",
    "ACT 3: Given a vector $\\mathbf{v} \\in \\mathbb{R}^d$, compute its projection onto the sphere $S_r = \\{ \\mathbf{u} \\in \\mathbb{R}^d \\, : \\, \\| \\mathbf{u} \\| \\leq r \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACT 3\n",
    "\n",
    "The projection of $\\mathbf{v}$ onto the sphere $S_r$ of radius $r$ would be given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{proj}\n",
    "=\n",
    "r\n",
    "\\frac\n",
    "{\\mathbf{v}}\n",
    "{||\\mathbf{v}||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericLoss:\n",
    "    def __init__(self, name=\"\", dims = [], W0 = []):\n",
    "        self.name = name\n",
    "        assert dims != [] or W0 != [], 'Must set dims or W0'\n",
    "        if W0 == []:\n",
    "            # self.W = np.zeros(dims) + 0.5\n",
    "            self.W = np.zeros(dims)\n",
    "        else:\n",
    "            self.W = W0\n",
    "        self.k, self.d = self.W.shape\n",
    "    \n",
    "    # set the parameter matrix to the given W\n",
    "    def Set(self, W):\n",
    "        \"\"\"\n",
    "        ACT 4\n",
    "        \"\"\"\n",
    "        self.W = W\n",
    "    \n",
    "    # get the parameter matrix of the instance\n",
    "    def Get(self):\n",
    "        \"\"\"\n",
    "        ACT 5\n",
    "        \"\"\"\n",
    "        return self.W\n",
    "    \n",
    "    # update the parameter matrix by adding given dW\n",
    "    def Update(self, dW):\n",
    "        \"\"\"\n",
    "        ACT 6\n",
    "        \"\"\"\n",
    "        self.W += dW  # assuming matrix shapes are compatible\n",
    "    \n",
    "    # compute the norms of the rows of the parameter matrix\n",
    "    def RowNorms(self):\n",
    "        \"\"\"\n",
    "        ACT 7\n",
    "        \"\"\"\n",
    "        return self.norm_rows(self.W)\n",
    "    \n",
    "    # compute the projection of each of the rows to the sphere of radius rad\n",
    "    def Project(self, rad):\n",
    "        \"\"\"\n",
    "        ACT 8\n",
    "        \"\"\"\n",
    "        scale = rad / self.RowNorms()\n",
    "        scale[scale > 1.0] = 1.0  # TO-DO: scale > 1 or scale > rad?\n",
    "        self.W *= scale\n",
    "    \n",
    "    # compute the numerical predictions (z values) given a data matrix X\n",
    "    def Predict(self, X):\n",
    "        \"\"\"\n",
    "        ACT 9\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Z : np.array(n x k)\n",
    "            A matrix of predictions Z.\n",
    "        \"\"\"\n",
    "        scores = np.einsum('ij,kj->ki', self.W, X)\n",
    "        return scores\n",
    "    \n",
    "    # compute the label predictions given a data matrix X\n",
    "    def PredictLabels(self, X):\n",
    "        \"\"\"\n",
    "        ACT 10\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        labels : np.array(1 x n)\n",
    "            A vector with the argmax of matrix Z, over all rows.\n",
    "        \"\"\"\n",
    "        labels = np.argmax(self.Predict(X), axis=1)  # or axis 1?\n",
    "        labels = labels.reshape((-1, 1))\n",
    "        return labels\n",
    "    \n",
    "    # compute the classification error given a data matrix X, and label vector y\n",
    "    def Error(self, X, y):\n",
    "        \"\"\"\n",
    "        ACT 11\n",
    "        \n",
    "        Error is the sum of all missed predictions. For each one of them, +1.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        e : scalar\n",
    "            The cumulative error.\n",
    "        \"\"\"\n",
    "        # y = y.reshape((1, -1))\n",
    "        labels = self.PredictLabels(X)\n",
    "        # e = (y != labels).astype(int)\n",
    "        e = (y != labels)\n",
    "        return np.sum(e) / labels.size\n",
    "        \n",
    "    @staticmethod\n",
    "    def norm_rows(A):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        norm : np.array(1 x dim_b)\n",
    "        \"\"\"\n",
    "        return np.linalg.norm(A, axis=1, keepdims=True)  # TO-DO: what shape should this be?\n",
    "\n",
    "    def normalize_rows(self, A):\n",
    "        \"\"\"\n",
    "        Normalizes the rows of a matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        B : np.array\n",
    "            Normalized copy of A.\n",
    "        \"\"\"\n",
    "        norm = self.norm_rows(A)\n",
    "        if np.amax(norm) == 0.0:\n",
    "            print('equal')\n",
    "            return A\n",
    "        return A / norm\n",
    "\n",
    "    def Loss(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def Gradient(self, X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the specific losses, you simply need to compute the loss value itself given $X \\in \\mathbb{R}^{n \\times d}, \\mathbf{y} \\in [k]^n$ and the parameter matrix $W \\in \\mathbb{R}^{k \\times d}$ as well as the gradient of that loss with respect to $W$. Let us first express $\\ell^{\\mathsf{CE}}$ and $\\ell^{\\mathsf{OvA}}$ in terms of $X, \\mathbf{y}, W$.\n",
    "\n",
    "$$\\ell^{\\mathsf{CE}}(X, \\mathbf{y}, W) = \\frac{1}{n} \\sum_{i=1}^n \\ell_{CE}(\\mathbf{y}_i, \\hat{\\mathbf{y}}_i), \\quad \\ell^{\\mathsf{OvA}}(X, \\mathbf{y}, W) = \\frac{1}{n} \\sum_{i=1}^n \\ell_{OvA}(\\bar{\\mathbf{y}}_i, \\mathbf{z}_i) ~ .$$\n",
    "\n",
    "where for each $i \\in [n]$ the vectors $\\mathbf{y}_i$ are the rows of $Y =$ `one_hot`$(\\mathbf{y}) \\in \\mathbb{R}^{n \\times k}$, $\\bar{\\mathbf{y}}_i$ are the rows of $\\bar{Y} = (2Y - 1) \\in \\mathbb{R}^{n \\times k}$, $\\mathbf{z}_i$ are the rows of $\\mathbf{Z} = X W^{\\top} \\in \\mathbb{R}^{n \\times d}$, and $\\hat{\\mathbf{y}}_i = \\exp(\\mathbf{z}_i)/Z_i$ with $Z_i$ making sure $\\hat{\\mathbf{y}}_i \\in \\Delta^k$. The gradient computation can be done step-by-step using the chain rule.\n",
    "\n",
    "ACT 12: Compute the gradient of $\\ell^{\\mathsf{CE}}$ w.r.t. $W$, given by $\\nabla_W \\ell^{\\mathsf{CE}}$.\n",
    "\n",
    "Hint: First, show for a single $\\mathbf{y}, \\mathbf{\\hat{y}}, \\mathbf{z}$ that $\\nabla_{\\mathbf{z}} \\ell_{CE}(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\mathbf{\\hat{y}} - \\mathbf{y}$ (simplify the loss function explicitly in terms of $\\mathbf{z}$ to make things easier). Afterwards, denoting $\\hat{Y} \\in \\mathbb{R}^{n \\times k}$ the matrix with rows $\\mathbf{\\hat{y}}_i$ for all $i \\in [n]$, conclude that $\\nabla_W \\ell^{\\mathsf{CE}}(X, \\mathbf{y}, W) = \\frac1n (\\hat{Y} - Y)^{\\top} X$.\n",
    "\n",
    "ACT 13: Compute the gradient of $\\ell^{\\mathsf{OvA}}$ w.r.t. $W$, given by $\\nabla_W \\ell^{\\mathsf{OvA}}$.\n",
    "\n",
    "Hint: First, show for a single $\\mathbf{\\bar{y}}, \\mathbf{z}$ that $\\nabla_{\\mathbf{z}} \\ell_{OvA}(\\mathbf{\\hat{y}}, \\mathbf{z}) = - \\frac{\\mathbf{\\hat{y}}}{1 + \\exp(\\mathbf{\\hat{y}} \\mathbf{z})}$ with all the operations in this expression being elementwise. Afterwards, conclude that $\\nabla_W \\ell^{\\mathsf{OvA}}(X, \\mathbf{y}, W) = - \\frac1n \\left( \\frac{\\bar{Y}}{1 + \\exp(\\bar{Y} \\mathbf{Z})} \\right)^{\\top} X$ again operations like multiplication, exponential division being elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(GenericLoss):\n",
    "\n",
    "    # In CE, the one-hot vectors should be smoothed with the given eps parameter.\n",
    "    def __init__(self, name=\"CrossEntropy\", dims=[], W0=[], eps=1e-6):\n",
    "        super().__init__(name, dims, W0)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def Loss(self, X, y, verbose=False):\n",
    "        \"\"\"\n",
    "        ACT 14\n",
    "        Compute the cross-entropy loss.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array(n x d)\n",
    "            The data matrix.\n",
    "        y : np.array(1 x n)\n",
    "            The labels vector.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        loss : scalar\n",
    "            The value of the cross-entropy loss.\n",
    "        \"\"\"\n",
    "        Y = one_hot(y, k=self.k, eps=self.eps)  # n x k\n",
    "                \n",
    "        Y_hat = self.calculate_Y_hat(X, verbose=verbose)\n",
    "        \n",
    "        print('Yshape', Y.shape)\n",
    "        print('Y hat shape', Y_hat.shape)\n",
    "\n",
    "        loss = Y * np.log(Y / Y_hat)\n",
    "        loss_ce = np.sum(loss, axis=1)     \n",
    "        loss_CE = np.average(loss_ce)\n",
    "        \n",
    "        if verbose:\n",
    "            print('total Y', np.sum(Y))\n",
    "            print('total Y hat', np.sum(Y_hat))\n",
    "            print('total loss', np.sum(loss))\n",
    "            print('total loss ce', np.sum(loss_ce))\n",
    "            print('total loss ce', np.sum(loss_CE))\n",
    "        return loss_CE\n",
    "    \n",
    "    def Gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        ACT 15\n",
    "        \n",
    "        Compute the gradient of the cross-entropy loss w.r.t to W.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array(n x d)\n",
    "            The data matrix.\n",
    "        y : np.array(1 x n)\n",
    "            The labels vector.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        grad : np.array(k x d)\n",
    "            The matrix gradient of the cross-entropy loss function.\n",
    "        \"\"\"\n",
    "        n, d = X.shape\n",
    "        Y = one_hot(y, k=self.k, eps=self.eps)  # n x k\n",
    "        \n",
    "        Y_hat = self.calculate_Y_hat(X)\n",
    "        Y_temp = Y_hat - Y\n",
    "        \n",
    "        # (1 / n) * (Y_matrix - Yhat_matrix)^T * X\n",
    "        grad = np.einsum('ij,ik->jk', Y_temp, X)  # k x d\n",
    "        # grad = np.matmul(np.transpose(Y_temp), X)\n",
    "        return  grad / n\n",
    "\n",
    "    def calculate_Y_hat(self, X, verbose=False):\n",
    "        \"\"\"\n",
    "        Computes the Y_hat matrix.\n",
    "        \n",
    "        Returns\n",
    "        ------\n",
    "        Y_hat : np.array(n x k)\n",
    "            The Y_hat matrix.\n",
    "        \"\"\"\n",
    "        Z = self.Predict(X)  # n x k  \n",
    "        labels = self.PredictLabels(X)\n",
    "        Z -= labels\n",
    "    \n",
    "        Y_hat_1 = np.exp(Z)  # n x k\n",
    "        Z_sum = np.sum(Y_hat_1, axis=1).reshape((-1, 1))\n",
    "    \n",
    "        Y_hat = Y_hat_1 / Z_sum # constrain to simplex  ### check this! before commenting it, tests did not pass\n",
    "        # Y_hat = Y_hat_1 / np.average(Y_hat_1, axis=0)  # constrain to simplex  ### check this! before commenting it, tests did not pass\n",
    "        \n",
    "        if verbose:\n",
    "            print('****Inside Y hat****')\n",
    "            print('Z', Z)\n",
    "            print('total Z', np.sum(Z))\n",
    "            print('Y hat non div', Y_hat_1)\n",
    "            print('total Y hat not div', np.sum(Y_hat_1))\n",
    "            \n",
    "            print('Y hat div', Y_hat)\n",
    "            print('total Y hat not div', np.sum(Y_hat))\n",
    "            print('****Leaving Y hat****')\n",
    "        return Y_hat\n",
    "\n",
    "    \n",
    "class LogisticOvA(GenericLoss):\n",
    "    def __init__(self, name=\"LogisticOvA\", dims=[], W0=[]):\n",
    "        super().__init__(name, dims, W0)\n",
    "\n",
    "    def Loss(self, X, y):\n",
    "        \"\"\"\n",
    "        ACT 16\n",
    "        \n",
    "        Computes the one-versus-all loss. Operations are element-wise.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array(n x d)\n",
    "            The data matrix.\n",
    "        y : np.array(1 x n)\n",
    "            The labels vector.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        loss : scalar\n",
    "            The value of the cross-entropy loss.\n",
    "        \"\"\"\n",
    "        Y = one_hot(y, k=self.k)  # n x k\n",
    "        Y_dash = self.calculate_Y_dash(Y)\n",
    "        \n",
    "        Z = self.Predict(X)  # n x k\n",
    "        #  Z = self.normalize_rows(Z)  # TO-DO: Test with and without normalization\n",
    "\n",
    "        power = -1 * Y_dash * Z\n",
    "        loss = np.log(1 + np.exp(power))\n",
    "        loss_ova = np.sum(loss, axis=1)\n",
    "        loss_OVA = np.average(loss_ova)\n",
    "        return loss_OVA\n",
    "        \n",
    "    def Gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        ACT 17\n",
    "        \n",
    "        Compute the gradient of the  one-versus-all loss w.r.t to W.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array(n x d)\n",
    "            The data matrix.\n",
    "        y : np.array(1 x n)\n",
    "            The labels vector.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        grad : np.array(k x d)\n",
    "            The matrix gradient of the cross-entropy loss function.\n",
    "        \"\"\"\n",
    "        n, d = X.shape\n",
    "        Y = one_hot(y, k=self.k)  # n x k\n",
    "        Y_dash = self.calculate_Y_dash(Y)\n",
    "        Z = self.Predict(X)  # n x k\n",
    "        #  Z = self.normalize_rows(Z)  # TO-DO: Test with and without normalization\n",
    "        \n",
    "        A = 1 + np.exp(Y_dash * Z)\n",
    "        B = Y_dash / A\n",
    "        grad =  -1 * np.einsum('ij,ik->jk', B, X) / n\n",
    "        return grad\n",
    "    \n",
    "    def calculate_Y_dash(self, Y):\n",
    "        \"\"\"\n",
    "        Computes the Y_dash matrix.\n",
    "        \n",
    "        Returns\n",
    "        ------\n",
    "        Y_dash : np.array(n x k)\n",
    "            The Y_dash matrix.\n",
    "        \"\"\"\n",
    "        return 2 * Y - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the two loss classes, we will test them the following way:\n",
    "\n",
    "(i) the loss of random labels should be larger than the loss of the assigned labels (the result of `PredictLabels`);\n",
    "\n",
    "(ii) the gradient with random labels should have a bigger norm than that with the assigned labels;\n",
    "\n",
    "(iii) the gradient norm should decrease after a single small gradient step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Test Passed\n",
      "Logistic OvA Test Passed\n"
     ]
    }
   ],
   "source": [
    "# assert the 3 points above for the loss class given by loss_tested\n",
    "# after implementing make sure both CrossEntropy, LogisticOvA pass all tests\n",
    "def TestLoss(loss_tested):\n",
    "    n, d, k, tests = 100, 10, 7, 1000\n",
    "    test_loss = loss_tested(W0=[], dims=(k, d))\n",
    "    \n",
    "    # the number of tests is given by tests.\n",
    "    # for each test, generate random Gaussian matrices X, W of appropriate sizes.\n",
    "    for _ in range(tests):\n",
    "        X, W = np.random.randn(n, d), np.random.randn(k, d)\n",
    "        test_loss.Set(W)\n",
    "    \n",
    "        # assert that the loss value with the assigned labels y is smaller than that\n",
    "        # with labels uniformly random from the interval [0, k-1].\n",
    "        y, y_rand = test_loss.PredictLabels(X), np.random.randint(0, k, n).reshape((n, -1))\n",
    "        ### ACT 18: loss value on X with assigned labels y\n",
    "        loss1 = test_loss.Loss(X, y)\n",
    "        ### ACT 19: loss value on X with random labels y_rand\n",
    "        loss2 = test_loss.Loss(X, y_rand)\n",
    "        assert loss1 < loss2, \"Loss test failed (%f >= %f)\" % (loss1, loss2)\n",
    "    \n",
    "        # assert that the gradient norm with the assigned labels is smaller than that\n",
    "        # with labels uniformly random from the interval [0, k-1].\n",
    "        grad1 = test_loss.Gradient(X, y)\n",
    "        ### ACT 20: norm of the gradient with X and assigned labels y\n",
    "        norm_grad1 = np.linalg.norm(grad1, ord='fro')\n",
    "        ### ACT 21: norm of the gradient with X and random labels y_rand\n",
    "        grad2 = test_loss.Gradient(X, y_rand)\n",
    "        norm_grad2 = np.linalg.norm(grad2, ord='fro')\n",
    "        assert norm_grad1 < norm_grad2, \"Gradient norm test failed (%f >= %f)\" % (norm_grad1, norm_grad2)\n",
    "        \n",
    "        # assert that after making a single gradient step (in the opposite direction)\n",
    "        # the gradient norm decreases (choose a small step size).\n",
    "        test_loss.Update(-0.01 * grad1)\n",
    "        grad3 = test_loss.Gradient(X, y)\n",
    "        ### ACT 22: norm of the gradient with X and y after making a single gradient step\n",
    "        norm_grad3 = np.linalg.norm(grad3, ord='fro')\n",
    "        assert norm_grad3 < norm_grad1, \"Gradient step test failed (%f >= %f)\" % (norm_grad3, norm_grad1)\n",
    "    \n",
    "    return True\n",
    "\n",
    "LossTested = CrossEntropy\n",
    "if TestLoss(LossTested): print('CE Test Passed')\n",
    "\n",
    "LossTested = LogisticOvA\n",
    "if TestLoss(LossTested): print('Logistic OvA Test Passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are methods from previous assignments for data processing and training. No need to reimplement these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a mini-batch w/ or w/o replacement\n",
    "from numpy.random import randint\n",
    "from numpy.random import permutation\n",
    "class IndexSampler:\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "        self.prm = None\n",
    "    \n",
    "    def sample_new_index(self, replace=0):\n",
    "        if replace:\n",
    "            return randint(self.d)\n",
    "        if self.prm is None:\n",
    "            self.prm = permutation(self.d)\n",
    "            self.head = 0\n",
    "        ind = self.prm[self.head]\n",
    "        self.head += 1\n",
    "        if self.head == self.d:\n",
    "            self.head = 0\n",
    "            self.prm = None\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector of learning-rate values. Mode can be: 'fixed_t', 'linear_t', 'sqrt_t'\n",
    "# Internal shift_t parameter can/should be changed during experiments.\n",
    "def learning_rate_schedule(eta0, epochs, mode):\n",
    "    base_t = 10.0\n",
    "    if mode == 'fixed_t':\n",
    "        return eta0 * np.ones(epochs)\n",
    "    if mode == 'sqrt_t':\n",
    "        return eta0 * np.ones(epochs) / (base_t + np.sqrt(np.arange(epochs)))\n",
    "    if mode == 'linear_t':\n",
    "        return eta0 * np.ones(epochs) / (base_t + np.arange(epochs))\n",
    "    print('invalid mode for learning rate schedule: %s' % mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_logistic_sgd_handle(batch_size, epochs, eta0, lr_mode, rad, replace, eps=None):\n",
    "    \"\"\"\n",
    "    Assembles parameters to train SGD algorithm.\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    handle : dict\n",
    "        The parameters dictionary.\n",
    "    \"\"\"\n",
    "    h = {}\n",
    "    h['pstr'] = 'Epoch: {0:2d}  Loss: {1:5.3f}  Error: {2:5.3f} %'\n",
    "    \n",
    "    h['rad'] = rad\n",
    "    h['replace'] = replace\n",
    "    h['eta0'] = eta0\n",
    "    \n",
    "    h['epochs'] = epochs\n",
    "    h['batch_size'] = batch_size\n",
    "    \n",
    "    h['lr_mode'] = lr_mode\n",
    "    h['eps'] = eps\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with general loss class. h is the handle defined above.\n",
    "def SGD(X, y, Loss, params):\n",
    "    h = params\n",
    "    \n",
    "    pstr, rad, replace = h['pstr'], h['rad'], ['replace']\n",
    "    eta0, epochs, bs, lrmode = h['eta0'], h['epochs'], h['batch_size'], h['lr_mode']\n",
    "    \n",
    "    n, d = X.shape\n",
    "    nbs = int(n / bs)\n",
    "    k = np.amax(y) + 1\n",
    "    \n",
    "    ls = Loss(W0=[], dims=(k, d))\n",
    "    eps = h.get('eps')\n",
    "    if eps:\n",
    "        ls.eps = eps\n",
    "        \n",
    "    eta_t = learning_rate_schedule(eta0, epochs, lrmode)\n",
    "    losses = [ls.Loss(X, y, verbose=False)]\n",
    "    errors = [ls.Error(X, y)]\n",
    "    sampler = IndexSampler(nbs)\n",
    "\n",
    "    print('*** Logistic Regression SGD Began ***\\n')\n",
    "    print(pstr.format(0, losses[-1], errors[-1] * 100))\n",
    "\n",
    "    for e in range(1, epochs * nbs):\n",
    "        head = sampler.sample_new_index(replace) * bs\n",
    "        Xt, yt = X[head:head + bs], y[head:head + bs]\n",
    "        gw = ls.Gradient(Xt, yt)\n",
    "        ls.Update(-eta_t[e // nbs] * gw)\n",
    "    \n",
    "        if rad > 0:\n",
    "            ls.Project(rad)\n",
    "        \n",
    "        if e % nbs == 0:\n",
    "            losses.append(ls.Loss(X, y))\n",
    "            errors.append(ls.Error(X, y))\n",
    "        \n",
    "        if (e % (nbs * 10)) == 0:\n",
    "            print(pstr.format(e // nbs, losses[-1], errors[-1] * 100))\n",
    "\n",
    "    return ls, losses, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib and get the mnist dataset from tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist as keras_mnist\n",
    "(X_train, y_train), (X_test, y_test) = keras_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data by subtracting the mean and dividing by std\n",
    "def normalize(X, bias=0):\n",
    "    n, d = X.shape\n",
    "    m = np.mean(X, axis=1).reshape(n, 1) * np.ones((1, d))\n",
    "    s = np.std(X, axis=1).reshape(n, 1) * np.ones((1, d))\n",
    "    Xn = (X - m) / s\n",
    "    if bias != 0:\n",
    "        Xn = np.hstack((Xn, bias * np.ones((n, 1))))\n",
    "    return Xn\n",
    "\n",
    "# flatten the images into d-dimensional vectors for training\n",
    "def flatten_images(X):\n",
    "    s = X.shape\n",
    "    n = s[0]\n",
    "    d = np.prod(s[1:])\n",
    "    return X.reshape(n, d)\n",
    "\n",
    "Xtr = normalize(flatten_images(X_train), bias = 1)\n",
    "Xte = normalize(flatten_images(X_test), bias = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Logistic Regression SGD Began ***\n",
      "\n",
      "Epoch:  0  Loss: 2.303  Error: 90.128 %\n",
      "Epoch: 10  Loss: 0.285  Error: 8.027 %\n",
      "Epoch: 20  Loss: 0.272  Error: 7.585 %\n",
      "Epoch: 30  Loss: 0.264  Error: 7.287 %\n",
      "Epoch: 40  Loss: 0.261  Error: 7.230 %\n",
      "Epoch: 50  Loss: 0.257  Error: 7.027 %\n",
      "Epoch: 60  Loss: 0.255  Error: 7.040 %\n",
      "Epoch: 70  Loss: 0.252  Error: 6.922 %\n",
      "Epoch: 80  Loss: 0.250  Error: 6.888 %\n",
      "Epoch: 90  Loss: 0.248  Error: 6.817 %\n",
      "Epoch: 100  Loss: 0.247  Error: 6.787 %\n",
      "Epoch: 110  Loss: 0.245  Error: 6.785 %\n",
      "Epoch: 120  Loss: 0.244  Error: 6.675 %\n",
      "Epoch: 130  Loss: 0.243  Error: 6.750 %\n",
      "Epoch: 140  Loss: 0.243  Error: 6.665 %\n",
      "Epoch: 150  Loss: 0.241  Error: 6.640 %\n",
      "Epoch: 160  Loss: 0.241  Error: 6.635 %\n",
      "Epoch: 170  Loss: 0.240  Error: 6.578 %\n",
      "Epoch: 180  Loss: 0.239  Error: 6.543 %\n",
      "Epoch: 190  Loss: 0.239  Error: 6.498 %\n",
      "Epoch: 200  Loss: 0.238  Error: 6.482 %\n",
      "Epoch: 210  Loss: 0.238  Error: 6.478 %\n",
      "Epoch: 220  Loss: 0.238  Error: 6.507 %\n",
      "Epoch: 230  Loss: 0.237  Error: 6.460 %\n",
      "Epoch: 240  Loss: 0.236  Error: 6.432 %\n",
      "Epoch: 250  Loss: 0.236  Error: 6.450 %\n",
      "Epoch: 260  Loss: 0.235  Error: 6.487 %\n",
      "Epoch: 270  Loss: 0.234  Error: 6.423 %\n",
      "Epoch: 280  Loss: 0.235  Error: 6.470 %\n",
      "Epoch: 290  Loss: 0.234  Error: 6.432 %\n",
      "Epoch: 300  Loss: 0.233  Error: 6.412 %\n",
      "Epoch: 310  Loss: 0.233  Error: 6.387 %\n",
      "Epoch: 320  Loss: 0.233  Error: 6.422 %\n",
      "Epoch: 330  Loss: 0.233  Error: 6.400 %\n",
      "Epoch: 340  Loss: 0.232  Error: 6.332 %\n",
      "Epoch: 350  Loss: 0.232  Error: 6.400 %\n",
      "Epoch: 360  Loss: 0.232  Error: 6.343 %\n",
      "Epoch: 370  Loss: 0.231  Error: 6.352 %\n",
      "Epoch: 380  Loss: 0.231  Error: 6.318 %\n",
      "Epoch: 390  Loss: 0.231  Error: 6.307 %\n",
      "Epoch: 400  Loss: 0.231  Error: 6.318 %\n",
      "Epoch: 410  Loss: 0.230  Error: 6.295 %\n",
      "Epoch: 420  Loss: 0.230  Error: 6.307 %\n",
      "Epoch: 430  Loss: 0.230  Error: 6.335 %\n",
      "Epoch: 440  Loss: 0.230  Error: 6.315 %\n",
      "Epoch: 450  Loss: 0.230  Error: 6.283 %\n",
      "Epoch: 460  Loss: 0.229  Error: 6.305 %\n",
      "Epoch: 470  Loss: 0.229  Error: 6.268 %\n",
      "Epoch: 480  Loss: 0.229  Error: 6.258 %\n",
      "Epoch: 490  Loss: 0.229  Error: 6.278 %\n",
      "*** Elapsed time: 12.72 minutes ***\n"
     ]
    }
   ],
   "source": [
    "### ACT 23\n",
    "# run SGD with CrossEntropy loss on the MNIST training data\n",
    "# batch size should be 1000, sampling with no replacement\n",
    "# number of epochs should be 500, sphere radius for W is 10.0\n",
    "# learning rate mode is sqrt_t with eta=1.0\n",
    "\n",
    "epochs = 500  # 500\n",
    "batch_size = 1000\n",
    "rad = 10.0\n",
    "lr_mode = 'sqrt_t'\n",
    "eta0 = 1.0\n",
    "replace = False\n",
    "eps = None  # 1e-6\n",
    "loss_ce = CrossEntropy\n",
    "\n",
    "start = time.time()\n",
    "y_train_ce = y_train.reshape((-1, 1))\n",
    "handle_ce = prepare_logistic_sgd_handle(batch_size, epochs, eta0, lr_mode, rad, replace, eps)\n",
    "w_ce, loss_ce, error_ce = SGD(Xtr, y_train_ce, loss_ce, handle_ce)\n",
    "\n",
    "print('*** Elapsed time: {} minutes ***'.format(round(((time.time() - start) / 60.0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yshape (10000, 10)\n",
      "Y hat shape (10000, 10)\n",
      "CE Test Loss: 0.263  CE Test Error: 7.300 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpj/anaconda3/envs/cos324/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/arpj/anaconda3/envs/cos324/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "### ACT 24\n",
    "# compute test error and loss\n",
    "\n",
    "W0 = w_ce.W\n",
    "loss_ce_test = CrossEntropy(W0=W0)\n",
    "\n",
    "y_test_ce = y_test.reshape((-1, 1))\n",
    "test_loss_ce = loss_ce_test.Loss(Xte, y_test_ce)\n",
    "test_err_ce = loss_ce_test.Error(Xte, y_test_ce)\n",
    "\n",
    "msg = 'CE Test Loss: {0:5.3f}  CE Test Error: {1:5.3f} %'\n",
    "print(msg.format(test_loss_ce, test_err_ce * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(fname, data_list, lnames=None):\n",
    "    for data in data_list:\n",
    "        plt.plot(range(1, 1 + len(data)), data, linewidth=2, linestyle='-')\n",
    "\n",
    "    if lnames:\n",
    "        plt.legend(tuple(lnames))\n",
    "    \n",
    "    plt.grid()    \n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel(fname, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### ACT 25\n",
    "# plot loss vs epochs, separately plot error vs epochs\n",
    "plotter('Loss', [loss_ce], ['loss SGD CE'])\n",
    "plotter('Error', [error_ce], ['error SGD CE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a8a2df9fd0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# C[i][j] indicates pct digit i was classified as j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# assert that each digit is most likely to be correctly classified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "### ACT 26\n",
    "# using test data construct a confusion matrix C that is 10x10\n",
    "# C[i][j] indicates pct digit i was classified as j\n",
    "# assert that each digit is most likely to be correctly classified\n",
    "assert np.array_equal(np.argmax(C, axis=1), np.arange(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 27\n",
    "# perform ACT 23-25 with LogisticOvA instead of CrossEntropy\n",
    "# compare your results, conclude in 1 sentence\n",
    "\n",
    "epochs = 500  # 500\n",
    "batch_size = 1000\n",
    "rad = 10.0\n",
    "lr_mode = 'sqrt_t'\n",
    "eta0 = 1.0\n",
    "replace = False\n",
    "loss_ce = LogisticOvA\n",
    "\n",
    "start = time.time()\n",
    "y_train_ce = y_train.reshape((-1, 1))\n",
    "handle_ce = prepare_logistic_sgd_handle(batch_size, epochs, eta0, lr_mode, rad, replace, eps)\n",
    "w_ce, loss_ce, error_ce = SGD(Xtr, y_train_ce, loss_ce, handle_ce)\n",
    "\n",
    "print('*** Elapsed time: {} minutes ***'.format(round(((time.time() - start) / 60.0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing for 20 newsgroups datasets already done for you.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import Counter\n",
    "\n",
    "def get_20newsgroups_data():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "    newsgroups_test  = fetch_20newsgroups(subset='test')\n",
    "    return newsgroups_train, newsgroups_test\n",
    "\n",
    "def construct_vocabulary(data, vs):\n",
    "    vocab = Counter()\n",
    "    for text in data:\n",
    "        for word in text.split(' '):\n",
    "            vocab[word.lower()] += 1\n",
    "    word2index = dict(vocab.most_common(vs))\n",
    "    i = 0\n",
    "    for k in word2index.keys():\n",
    "        word2index[k] = i\n",
    "        i += 1\n",
    "    return word2index\n",
    "\n",
    "def text_to_vec(data, vocab):\n",
    "    def norm_rows(M):\n",
    "        return np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "    def project_rows(M, r):\n",
    "        return M * np.minimum(r / norm_rows(M), 1.0)\n",
    "    n = len(data)\n",
    "    d = len(vocab)\n",
    "    X = np.zeros((n, d))\n",
    "    i = 0\n",
    "    for text in data:\n",
    "        for word in text.split(' '):\n",
    "            if word.lower() in vocab:\n",
    "                X[i, vocab[word.lower()]] += 1.0\n",
    "        i += 1\n",
    "    # Convert to log-frequencies and normalize to have ||X[i,*]||=1\n",
    "    X = project_rows(np.log(X + 1.0), 1.0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsize = 1000\n",
    "train, test = get_20newsgroups_data()\n",
    "vocab = construct_vocabulary(train.data, vsize)\n",
    "Xtr = text_to_vec(train.data, vocab)\n",
    "Xte = text_to_vec(test.data, vocab)\n",
    "y_train = train.target\n",
    "y_test = test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will perform the same experiments with this dataset\n",
    "### ACT 28\n",
    "# perform ACT 23-27 for the 20newsgroups dataset\n",
    "# all parameters for training should be the same except:\n",
    "# we will start with a much larger learning rate eta=1000.0\n",
    "# and a sphere radius 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 29\n",
    "# after training, testing, and plotting make sure to compute the\n",
    "# row norms of the final parameters (mean and std), and conclude \n",
    "# in one sentence whether the results were as you expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
