{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 - Multiclass Learning\n",
    "\n",
    "In this exercise you are required to design, implement, train, and test multiclass learning algorithms for the [MNIST](http://yann.lecun.com/exdb/mnist) dataset of images and the [20 Newsgroups](http://qwone.com/~jason/20Newsgroups) dataset of text news data. You will explore two different approaches to multiclass learning: *Cross Entropy*, a general loss appropriate for multiple classes, and *One-vs-All*, a reduction scheme from binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have the packages numpy, scikit-learn, and Tensorflow\n",
    "# installed before starting the assignment\n",
    "\n",
    "# import numpy first\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Losses\n",
    "\n",
    "In the case of Cross Entropy, denoted in short as **CE**, the supervision for an example $\\mathbf{x}\\in\\mathbb{R}^d$  is, instead of a label\n",
    "$y\\in[k]\\stackrel{\\tiny \\mathsf{def}}{=}\\{1,\\ldots,k\\}$, a vector $\\mathbf{y}\\in\\Delta^k$ where\n",
    "$$\\Delta^k = \\big\\{\\mathbf{p}=(p_1,\\ldots,p_k)\\, \\big| \\, \\sum_i p_i = 1\n",
    "   \\mbox{ and } \\forall i: p_i \\geq 0 \\big\\} ~.$$\n",
    "$\\Delta^k$ is called the k$^{th}$ dimensional simplex and, informally speaking, it consists of all probability vectors in $\\mathbb{R}^k$. The CE loss for a target probability vector $\\mathbf{y}$ and predicted probability vector $\\mathbf{\\hat{y}}$ is defined as follows,\n",
    "   $$\\ell_{CE}(\\mathbf{y, \\hat{y}}) \\stackrel{\\tiny \\mathsf{def}}{=}\n",
    "     \\sum_{i=1}^k y[i] \\log\\left(\\frac{y[i]}{\\hat{y}[i]}\\right) ~ .$$\n",
    "We use the convention that $0\\log(0)\\stackrel{\\tiny \\mathsf{def}}{=} 0$. To obtain the vector $\\mathbf{\\hat{y}}$ we exponentiate the vector of predictions $\\mathbf{z} = W\\mathbf{x} \\in \\mathbb{R}^k$ where $\\mathbf{\\hat{y}}[i] = \\frac{\\exp({\\mathbf{z}[i]})}{Z}$ for all $i \\in [k]$ and $Z$ ensures that $\\mathbf{\\hat{y}}\\in \\Delta^k $.\n",
    "\n",
    "The One-vs-All, or **OvA**, approach instead uses $k$ logistic binary classifiers: the idea is to pick the label that has been *most confidently* chosen for the given example versus the rest. Formally, the loss can be expressed as follows. Given $(\\mathbf{x},y)\\in\\mathbb{R}^d\\times[k]$ let us define $\\mathbf{\\bar{y}}\\in\\{-1,1\\}^k$ as the vector of labels resulting from the multiclass reduction. That is, $\\mathbf{\\bar{y}}[y]=1$ and for all $j\\neq y$ we set $\\mathbf{\\bar{y}}[j]=-1$. Then the loss for $(\\mathbf{x},y)$, with $\\mathbf{z}$ defined as before, is\n",
    "$$\n",
    "\\ell_{OvA}(\\mathbf{\\bar{y}}, \\mathbf{z}) = \\sum_{j=1}^k \\log\\big(1+ e^{-\\mathbf{\\bar{y}}[j] \\, \\mathbf{z}[j]}\\big) ~ .\n",
    "$$\n",
    "\n",
    "ACT 1: Compute the value of $Z$ explicitly. Propose a method of computing $\\mathbf{\\hat{y}}$ from $\\mathbf{z}$ that is numerically stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\in \\mathbb{R}^{n \\times d}$ be the given dataset and $\\mathbf{y} \\in [k]^n$ the corresponding labels. Construct one-hot vectors for each label to have a probability vector from $\\Delta^k$: the one-hot vector for the class $c \\in [k]$ if defined as $\\mathbf{1}_c \\in \\Delta^k$ which is a vector of zeros except for coordinate $c$ where it is $1$. For example, suppose $k=4$ and $c=2$, then $\\mathbf{1}_c$ is $(0, 1, 0, 0)$. When needed we will\n",
    "be smoothing the one-hot vectors for numerical purposes. The $\\epsilon$-smoothed one-hot vector for class $c$ is given by $(1-\\epsilon) \\mathbf{1}_c + \\epsilon \\mathbf{\\frac1k}$ where $\\frac1k \\in \\Delta^k$ is the uniform probability vector, namely $\\mathbf{\\frac1k} = (\\frac1k, \\frac1k, \\ldots, \\frac1k)$.\n",
    "\n",
    "ACT 2: Implement the `one_hot` function that given $\\mathbf{y} \\in [k]^n$ data of labels returns $Y \\in \\mathbb{R}^{n \\times k}$ matrix of one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given y vector of labels, return matrix Y whose i-th row is the one-hot vector of label y_i \n",
    "# If eps is not 0, then need to compute the smoothed one-hot vectors instead\n",
    "# If k is 0, then need to infer its value instead\n",
    "def one_hot(y, k=0, eps=0):\n",
    "  ### ACT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first implement an abstract class for losses in general that has all the methods implemented except computing the loss value and the gradient. Then each of CE and OvA classes will implement their specific loss functions, and gradients separately. *Feel free to add your own methods to the class skeletons provided.*\n",
    "\n",
    "It is worth to note that label prediction in both cases is done by predicting the coordinate with the largest value in the vector $\\mathbf{z}$ as the label. In order to prevent the parameters from blowing up during training, we will implement methods `RowNorms` and `Project` to compute the norms of the rows of $W \\in \\mathbb{R}^{k \\times d}$ and their projections onto the sphere of radius $r$.\n",
    "\n",
    "ACT 3: Given a vector $\\mathbf{v} \\in \\mathbb{R}^d$, compute its projection onto the sphere $S_r = \\{ \\mathbf{u} \\in \\mathbb{R}^d \\, : \\, \\| \\mathbf{u} \\| \\leq r \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericLoss:\n",
    "    def __init__(self, name=\"\", dims = [], W0 = []):\n",
    "        self.name = name\n",
    "        assert dims != [] or W0 != [], 'Must set dims or W0'\n",
    "        if W0 == []:\n",
    "            self.W = np.zeros(dims)\n",
    "        else:\n",
    "            self.W = W0\n",
    "        self.k, self.d = self.W.shape\n",
    "    \n",
    "    # set the parameter matrix to the given W\n",
    "    def Set(self, W):\n",
    "        ### ACT 4\n",
    "    \n",
    "    # get the parameter matrix of the instance\n",
    "    def Get(self):\n",
    "        ### ACT 5\n",
    "    \n",
    "    # update the parameter matrix by adding given dW\n",
    "    def Update(self, dW):\n",
    "        ### ACT 6\n",
    "    \n",
    "    # compute the norms of the rows of the parameter matrix\n",
    "    def RowNorms(self):\n",
    "        ### ACT 7\n",
    "    \n",
    "    # compute the projection of each of the rows to the sphere of radius rad\n",
    "    def Project(self, rad):\n",
    "        ### ACT 8\n",
    "    \n",
    "    # compute the numerical predictions (z values) given a data matrix X\n",
    "    def Predict(self, X):\n",
    "        ### ACT 9\n",
    "    \n",
    "    # compute the label predictions given a data matrix X\n",
    "    def PredictLabels(self, X):\n",
    "        ### ACT 10\n",
    "    \n",
    "    # compute the classification error given a data matrix X, and label vector y\n",
    "    def Error(self, X, y):\n",
    "        ### ACT 11\n",
    "        \n",
    "    def Loss(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def Gradient(self, X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the specific losses, you simply need to compute the loss value itself given $X \\in \\mathbb{R}^{n \\times d}, \\mathbf{y} \\in [k]^n$ and the parameter matrix $W \\in \\mathbb{R}^{k \\times d}$ as well as the gradient of that loss with respect to $W$. Let us first express $\\ell^{\\mathsf{CE}}$ and $\\ell^{\\mathsf{OvA}}$ in terms of $X, \\mathbf{y}, W$.\n",
    "\n",
    "$$\\ell^{\\mathsf{CE}}(X, \\mathbf{y}, W) = \\frac{1}{n} \\sum_{i=1}^n \\ell_{CE}(\\mathbf{y}_i, \\hat{\\mathbf{y}}_i), \\quad \\ell^{\\mathsf{OvA}}(X, \\mathbf{y}, W) = \\frac{1}{n} \\sum_{i=1}^n \\ell_{OvA}(\\bar{\\mathbf{y}}_i, \\mathbf{z}_i) ~ .$$\n",
    "\n",
    "where for each $i \\in [n]$ the vectors $\\mathbf{y}_i$ are the rows of $Y =$ `one_hot`$(\\mathbf{y}) \\in \\mathbb{R}^{n \\times k}$, $\\bar{\\mathbf{y}}_i$ are the rows of $\\bar{Y} = (2Y - 1) \\in \\mathbb{R}^{n \\times k}$, $\\mathbf{z}_i$ are the rows of $X W^{\\top} \\in \\mathbb{R}^{n \\times d}$, and $\\hat{\\mathbf{y}}_i = \\exp(\\mathbf{z}_i)/Z_i$ with $Z_i$ making sure $\\hat{\\mathbf{y}}_i \\in \\Delta^k$. The gradient computation can be done step-by-step using the chain rule.\n",
    "\n",
    "ACT 12: Compute the gradient of $\\ell^{\\mathsf{CE}}$ w.r.t. $W$, given by $\\nabla_W \\ell^{\\mathsf{CE}}$.\n",
    "\n",
    "ACT 13: Compute the gradient of $\\ell^{\\mathsf{OvA}}$ w.r.t. $W$, given by $\\nabla_W \\ell^{\\mathsf{OvA}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(GenericLoss):\n",
    "\n",
    "    # In CE, the one-hot vectors should be smoothed with the given eps parameter.\n",
    "    def __init__(self, eps=1e-6):\n",
    "        self.eps = eps\n",
    "        \n",
    "    # compute the CE loss as defined above.\n",
    "    def Loss(self, X, y):\n",
    "        ### ACT 14\n",
    "    \n",
    "    # compute the gradient w.r.t. W as done in ACT 13.\n",
    "    def Gradient(self, X, y):\n",
    "        ### ACT 15\n",
    "\n",
    "\n",
    "class LogisticOvA(GenericLoss):\n",
    "    \n",
    "    # compute the OvA loss as defined above.\n",
    "    def Loss(self, X, y):\n",
    "        ### ACT 16\n",
    "        \n",
    "    # compute the gradient w.r.t. W as done in ACT 14.\n",
    "    def Gradient(self, X, y):\n",
    "        ### ACT 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the two loss classes, we will test them the following way:\n",
    "\n",
    "(i) the loss of random labels should be larger than the loss of the assigned labels (the result of `PredictLabels`);\n",
    "\n",
    "(ii) the gradient with random labels should have a bigger norm than that with the assigned labels;\n",
    "\n",
    "(iii) the gradient norm should decrease after a single small gradient step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert the 3 points above for the loss class given by loss_tested\n",
    "# after implementing make sure both CrossEntropy, LogisticOvA pass all tests\n",
    "def TestLoss(loss_tested):\n",
    "    n, d, k, tests = 100, 10, 7, 1000\n",
    "    test_loss = loss_tested(W0=[], dims=(k, d))\n",
    "    \n",
    "    # the number of tests is given by tests.\n",
    "    # for each test, generate random Gaussian matrices X, W of appropriate sizes.\n",
    "    for _ in range(tests):\n",
    "        X, W = np.random.randn(n, d), np.random.randn(k, d)\n",
    "        test_loss.Set(W)\n",
    "    \n",
    "        # assert that the loss value with the assigned labels y is smaller than that\n",
    "        # with labels uniformly random from the interval [0, k-1].\n",
    "        y, y_rand = lt.PredictLabels(X), np.random.randint(0, k, n)\n",
    "        loss1 = ### ACT 18: loss value on X with assigned labels y\n",
    "        loss2 = ### ACT 29: loss value on X with random labels y_rand\n",
    "        assert loss1 < loss2, \"Loss test failed (%f >= %f)\" % (loss1, loss2)\n",
    "    \n",
    "        # assert that the gradient norm with the assigned labels is smaller than that\n",
    "        # with labels uniformly random from the interval [0, k-1].\n",
    "        grad1 = test_loss.Gradient(X, y)\n",
    "        norm_grad1 = ### ACT 20: norm of the gradient with X and assigned labels y\n",
    "        norm_grad2 = ### ACT 21: norm of the gradient with X and random labels y_rand\n",
    "        assert norm_grad1 < norm_grad2, \"Gradient norm test failed (%f >= %f)\" % (norm_grad1, norm_grad2)\n",
    "        \n",
    "        # assert that after making a single gradient step (in the opposite direction)\n",
    "        # the gradient norm decreases (choose a small step size).\n",
    "        test_loss.Update(-0.01 * grad1)\n",
    "        norm_grad3 = ### ACT 22: norm of the gradient with X and y after making a single gradient step\n",
    "        assert norm_grad3 < norm_grad1, \"Gradient step test failed (%f >= %f)\" % (norm_grad3, norm_grad1)\n",
    "    \n",
    "    return True\n",
    "\n",
    "LossTested = CrossEntropy\n",
    "if TestLoss(LossTested): print('CE Test Passed')\n",
    "LossTested = LogisticOvA\n",
    "if TestLoss(LossTested): print('Logistic OvA Test Passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are methods from previous assignments for data processing and training. No need to reimplement these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a mini-batch w/ or w/o replacement\n",
    "from numpy.random import randint\n",
    "from numpy.random import permutation\n",
    "class IndexSampler:\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "        self.prm = None\n",
    "    \n",
    "    def sample_new_index(self, replace=0):\n",
    "        if replace:\n",
    "            return randint(self.d)\n",
    "        if self.prm is None:\n",
    "            self.prm = permutation(self.d)\n",
    "            self.head = 0\n",
    "        ind = self.prm[self.head]\n",
    "        self.head += 1\n",
    "        if self.head == self.d:\n",
    "            self.head = 0\n",
    "            self.prm = None\n",
    "        return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector of learning-rate values. Mode can be: 'fixed_t', 'linear_t', 'sqrt_t'\n",
    "# Internal shift_t parameter can/should be changed during experiments.\n",
    "def learning_rate_schedule(eta0, epochs, mode):\n",
    "    base_t = 10.0\n",
    "    if mode == 'fixed_t':\n",
    "        return eta0 * np.ones(epochs)\n",
    "    if mode == 'sqrt_t':\n",
    "        return eta0 * np.ones(epochs) / (base_t + np.sqrt(np.arange(epochs)))\n",
    "    if mode == 'linear_t':\n",
    "        return eta0 * np.ones(epochs) / (base_t + np.arange(epochs))\n",
    "    print('invalid mode for learning rate schedule: %s' % mode)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with general loss class. h is the handle defined above.\n",
    "def SGD(X, y, Loss, params):\n",
    "    h = params\n",
    "    eps, pstr, rad, replace = h['eps'], h['pstr'], h['rad'], ['replace']\n",
    "    eta0, epochs, bs, lrmode = h['eta0'], h['epochs'], h['batch_size'], h['lr_mode']\n",
    "    n, d = X.shape\n",
    "    nbs = int(n / bs)\n",
    "    k = max(y) + 1\n",
    "    ls = Loss(W0=[], dims=(k, d))\n",
    "    eta_t = learning_rate_schedule(eta0, epochs, lrmode)\n",
    "    losses = [ls.Loss(X, y)]\n",
    "    errors = [ls.Error(X, y)]\n",
    "    sampler = IndexSampler(nbs)\n",
    "    for e in range(1, epochs * nbs):\n",
    "        head = sampler.sample_new_index(replace) * bs\n",
    "        Xt, yt = X[head:head + bs], y[head:head + bs]\n",
    "        gw = ls.Gradient(Xt, yt)\n",
    "        ls.Update(-eta_t[e // nbs] * gw)\n",
    "        if rad > 0: ls.Project(rad)\n",
    "        if e % nbs == 0:\n",
    "            losses.append(ls.Loss(X, y))\n",
    "            errors.append(ls.Error(X, y))\n",
    "        if (e % (nbs * 10)) == 0:\n",
    "            print(pstr.format(e // nbs, losses[-1], errors[-1]))\n",
    "    return ls, losses, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib and get the mnist dataset from tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist as keras_mnist\n",
    "(X_train, y_train), (X_test, y_test) = keras_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data by subtracting the mean and dividing by std\n",
    "def normalize(X, bias=0):\n",
    "    n, d = X.shape\n",
    "    m = np.mean(X, axis=1).reshape(n, 1) * np.ones((1, d))\n",
    "    s = np.std(X, axis=1).reshape(n, 1) * np.ones((1, d))\n",
    "    Xn = (X - m) / s\n",
    "    if bias != 0:\n",
    "        Xn = np.hstack((Xn, bias * np.ones((n, 1))))\n",
    "    return Xn\n",
    "\n",
    "# flatten the images into d-dimensional vectors for training\n",
    "def flatten_images(X):\n",
    "    s = X.shape\n",
    "    n = s[0]\n",
    "    d = np.prod(s[1:])\n",
    "    return X.reshape(n, d)\n",
    "\n",
    "Xtr = normalize(flatten_images(X_train), bias = 1)\n",
    "Xte = normalize(flatten_images(X_test), bias = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 23\n",
    "# run SGD with CrossEntropy loss on the MNIST training data\n",
    "# batch size should be 1000, sampling with no replacement\n",
    "# number of epochs should be 500, sphere radius for W is 10.0\n",
    "# learning rate mode is sqrt_t with eta=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 24\n",
    "# compute test error and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 25\n",
    "# plot loss vs epochs, separately plot error vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 26\n",
    "# using test data construct a confusion matrix C that is 10x10\n",
    "# C[i][j] indicates pct digit i was classified as j\n",
    "# assert that each digit is most likely to be correctly classified\n",
    "assert np.array_equal(np.argmax(C, axis=1), np.arange(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 27\n",
    "# perform ACT 23-25 with LogisticOvA instead of CrossEntropy\n",
    "# compare your results, conclude in 1 sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing for 20 newsgroups datasets already done for you.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import Counter\n",
    "\n",
    "def get_20newsgroups_data():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "    newsgroups_test  = fetch_20newsgroups(subset='test')\n",
    "    return newsgroups_train, newsgroups_test\n",
    "\n",
    "def construct_vocabulary(data, vs):\n",
    "    vocab = Counter()\n",
    "    for text in data:\n",
    "        for word in text.split(' '):\n",
    "            vocab[word.lower()] += 1\n",
    "    word2index = dict(vocab.most_common(vs))\n",
    "    i = 0\n",
    "    for k in word2index.keys():\n",
    "        word2index[k] = i\n",
    "        i += 1\n",
    "    return word2index\n",
    "\n",
    "def text_to_vec(data, vocab):\n",
    "    def norm_rows(M):\n",
    "        return np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "    def project_rows(M, r):\n",
    "        return M * np.minimum(r / norm_rows(M), 1.0)\n",
    "    n = len(data)\n",
    "    d = len(vocab)\n",
    "    X = np.zeros((n, d))\n",
    "    i = 0\n",
    "    for text in data:\n",
    "        for word in text.split(' '):\n",
    "            if word.lower() in vocab:\n",
    "                X[i, vocab[word.lower()]] += 1.0\n",
    "        i += 1\n",
    "    # Convert to log-frequencies and normalize to have ||X[i,*]||=1\n",
    "    X = project_rows(np.log(X + 1.0), 1.0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsize = 1000\n",
    "train, test = get_20newsgroups_data()\n",
    "vocab = construct_vocabulary(train.data, vsize)\n",
    "Xtr = text_to_vec(train.data, vocab)\n",
    "Xte = text_to_vec(test.data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will perform the same experiments with this dataset\n",
    "### ACT 28\n",
    "# perform ACT 23-27 for the 20newsgroups dataset\n",
    "# all parameters for training should be the same except:\n",
    "# we will start with a much larger learning rate eta=1000.0\n",
    "# and a sphere radius 40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT 29\n",
    "# after training, testing, and plotting make sure to compute the\n",
    "# row norms of the final parameters (mean and std), and conclude \n",
    "# in one sentence whether the results were as you expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
